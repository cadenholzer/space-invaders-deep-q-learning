{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "777dd059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, Flatten, GlobalAvgPool2D\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9428d2d-e4fe-4739-ad35-71d193cdb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "    - Install and experiment with tensorflow-GPU and CUDA.\n",
    "    - Experiment with different network architectures / hyperparameters.\n",
    "    - Train agent for much longer and many more steps...\n",
    "    - Train multiple models and compare with matplotlib graphs...\n",
    "    - Add way to save from and load models to agent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3f0be9a5-27c3-454f-8a2e-9435e0423927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "8f966b57-266c-451f-a1f1-b7abca79e51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MemoryBuffer():\n",
    "    \"\"\"\n",
    "    Class used to hold the memory experiences of the agent. Data pertaining to each \"step\" of the environment is\n",
    "    contained within an \"experience\" tuple. A buffer holds these tuples, which the agent extracts during learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_mem_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_mem_size: (int) max size of memory buffer.\n",
    "            \n",
    "        Attributes:\n",
    "            experience: (namedtuple) represents a single \"experience\" / step of environment. Contains\n",
    "                the current state, action, reward, next state, and whether the episode is done or truncated.\n",
    "            buffer: deque used as structure to hold \"experiences\".\n",
    "        \"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"truncated\"])\n",
    "        self.buffer = deque(maxlen = max_mem_size)\n",
    "        \n",
    "    def store_memory(self,  state, action, reward, next_state, done, truncated):\n",
    "        \"\"\"\n",
    "        Adds a single \"experience\" to the memory buffer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.buffer.append(self.experience(state, action, reward, next_state, done, truncated))\n",
    "        \n",
    "    def get_experiences(self, batch_size):\n",
    "        \"\"\"\n",
    "        Extracts a random selection of experiences from the memory buffer. \n",
    "        \n",
    "        Args: \n",
    "            batch_size: (int) number of experiences to extract.\n",
    "            \n",
    "        Returns:\n",
    "            states: (numpy array) each element is a 210x60x3 array representing the current game image.\n",
    "            actions: (numpy array) array holding each action chosen for each corresponding step.\n",
    "            rewards: (numpy array) rewards gained from each corresponding step.\n",
    "            next_states: (numpy array) each element is a 210x60x3 array representing the next game image based on \n",
    "                the action taken.\n",
    "            dones: (numpy array) 1 if corresponding episode is finished, 0 otherwise.\n",
    "            truncateds: (numpy array) 1 if corresponding episode terminated, 0 otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        random_selection = random.sample(self.buffer, batch_size)\n",
    "        states = np.array([experience.state for experience in random_selection])\n",
    "        actions = np.array([experience.action for experience in random_selection])\n",
    "        rewards = np.array([experience.reward for experience in random_selection])\n",
    "        next_states = np.array([experience.next_state for experience in random_selection])\n",
    "        dones = np.array([experience.done for experience in random_selection])\n",
    "        truncateds = np.array([experience.truncated for experience in random_selection])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, truncateds\n",
    "        \n",
    "        \n",
    "class SpaceInvaderAgent():\n",
    "    \"\"\"\n",
    "    Represents an agent that can learn to play Atari Space Invaders using Deep-Q Learning. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learn_rate, gamma, num_actions, epsilon, batch_size, max_mem_size, epsilon_decay, num_steps_for_learn, epsilon_end = 0.01, update_target_net = 100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            TODO: Add args...\n",
    "            \n",
    "        Attributes:\n",
    "            TODO: Add attributes...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.actions = [i for i in range(num_actions)]\n",
    "        self.learn_rate = learn_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = MemoryBuffer(max_mem_size)\n",
    "        self.num_steps_for_learn = num_steps_for_learn\n",
    "        self.learn_step_cntr = 0\n",
    "        self.update_target_net = update_target_net\n",
    "        \n",
    "        self.q_network = Sequential([\n",
    "            Input(shape = (210, 160, 3)),\n",
    "            Conv2D(filters = 32, kernel_size = 3, activation = 'relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 64, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 128, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(num_actions, activation='linear')\n",
    "            ])\n",
    "        \n",
    "        self.q_target = Sequential([\n",
    "            Input(shape = (210, 160, 3)),\n",
    "            Conv2D(filters = 32, kernel_size = 3, activation = 'relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 64, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 128, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(num_actions, activation='linear')\n",
    "            ])\n",
    "        \n",
    "        self.optimizer = Adam(learning_rate = self.learn_rate)\n",
    "        self.q_network.compile()\n",
    "        self.q_target.compile()\n",
    "                               \n",
    "        \n",
    "    def store_memory(self, state, action, reward, next_state, done, truncated):\n",
    "        \"\"\"\n",
    "        Adds a single \"experience\" to the agent's MemoryBuffer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.store_memory(state, action, reward, next_state, done, truncated)\n",
    "                              \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Chooses an action based on the agent's greedy-epsilon policy. Possible actions include: \n",
    "        0 (NOOP), 1 (FIRE), 2 (RIGHT), 3 (LEFT), 4 (RIGHTFIRE), 5 (LEFTFIRE)\n",
    "        \n",
    "        Args:\n",
    "            TODO: add state arg...\n",
    "            \n",
    "        Returns: \n",
    "            action: (int) representing one of the six possible actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pick a random action if generated random number is less than epsilon.\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        # Use Q-Network to pick action based on state.\n",
    "        else:\n",
    "            q_values = self.q_network(state)\n",
    "            action = tf.math.argmax(q_values, axis = 1).numpy()[0]\n",
    "                              \n",
    "        return action\n",
    "                              \n",
    "    def compute_loss(self, states, actions, rewards, next_states, dones, truncateds):\n",
    "        \"\"\"\n",
    "        Calculates the loss for the Deep-Q Network.\n",
    "\n",
    "        Args:\n",
    "            states: (numpy array) each element is a 210x60x3 array representing the current game image.\n",
    "            actions: (numpy array) array holding each action chosen for each corresponding step.\n",
    "            rewards: (numpy array) rewards gained from each corresponding step.\n",
    "            next_states: (numpy array) each element is a 210x60x3 array representing the next game image based on \n",
    "                the action taken.\n",
    "            dones: (numpy array) 1 if corresponding episode is finished, 0 otherwise.\n",
    "            truncateds: (numpy array) 1 if corresponding episode terminated, 0 otherwise.  \n",
    "\n",
    "        Returns:\n",
    "            loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between the y targets and the Q(s,a) values.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Determine max-Q(s, a) from next states and calculate y-targets using the target Q-Network.\n",
    "        max_qsa = tf.reduce_max(self.q_target(next_states), axis=-1)\n",
    "        y_targets = rewards + (1 - dones) * (1 - truncateds) * self.gamma * max_qsa\n",
    "        \n",
    "        # Determine Q-values from chosen actions using the original Q-Network.\n",
    "        q_values = self.q_network(states)\n",
    "        q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "        # Calculate the loss from the Mean-Squared Error between the y targest and Q-values.\n",
    "        loss = MSE(y_targets, q_values)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Updates the trainable weights of the Q-Network based on the Mean-Sqaured Error loss function. Learning only\n",
    "        takes place every \"num_steps_for_learn\" steps. Will not learn unless the MemoryBuffer contains enough\n",
    "        experiences for a full sample batch (batch_size). Also decrements the agent's epsilon attribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update learn step counter.\n",
    "        self.learn_step_cntr += 1\n",
    "        \n",
    "        # Decrement epsilon.\n",
    "        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.epsilon_end else self.epsilon\n",
    "        \n",
    "        # Check if it is time to learn.\n",
    "        if self.learn_step_cntr % self.num_steps_for_learn != 0:\n",
    "            return\n",
    "        \n",
    "        # Make sure MemoryBuffer has enough experiences for full batch.\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Update target Q-Network based on step count.\n",
    "        if self.learn_step_cntr % self.update_target_net == 0:\n",
    "            self.q_target.set_weights(self.q_network.get_weights())\n",
    "                              \n",
    "        # Extract experiences from MemoryBuffer.    \n",
    "        states, actions, rewards, next_states, dones, truncateds = self.memory.get_experiences(self.batch_size)\n",
    "            \n",
    "        # Compute loss with GradientTape.    \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(states, actions, rewards, next_states, dones, truncateds)\n",
    "\n",
    "        # Compute the gradients of the loss with respect to the weights.\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "\n",
    "        # Update the weights of the q_network.\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "                              \n",
    "    \n",
    "    def save_model(self):\n",
    "        #TODO: Add way to save q_network.\n",
    "        return\n",
    "    \n",
    "    def load_model(self):\n",
    "        #TODO: Add way to load an existing q_network.\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "e535550b-1806-4453-bc7f-474b09e8fd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Total points: 200.00 | Duration: 196.71 | Total Steps: 500 | Current Epsilon: 0.95\n",
      "Episode 2 | Total points: 75.00 | Duration: 225.43 | Total Steps: 1000 | Current Epsilon: 0.90\n",
      "Episode 3 | Total points: 180.00 | Duration: 227.99 | Total Steps: 1500 | Current Epsilon: 0.85\n",
      "Episode 4 | Total points: 155.00 | Duration: 226.78 | Total Steps: 2000 | Current Epsilon: 0.80\n",
      "Episode 5 | Total points: 135.00 | Duration: 230.82 | Total Steps: 2500 | Current Epsilon: 0.75\n",
      "Episode 6 | Total points: 175.00 | Duration: 229.51 | Total Steps: 3000 | Current Epsilon: 0.70\n",
      "Episode 7 | Total points: 55.00 | Duration: 172.66 | Total Steps: 3383 | Current Epsilon: 0.66\n",
      "Episode 8 | Total points: 180.00 | Duration: 227.42 | Total Steps: 3883 | Current Epsilon: 0.61\n",
      "Episode 9 | Total points: 135.00 | Duration: 227.22 | Total Steps: 4383 | Current Epsilon: 0.56\n",
      "Episode 10 | Total points: 130.00 | Duration: 229.78 | Total Steps: 4872 | Current Epsilon: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Create SpaceInvader gym environment with rgb_array mode.\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space = False, render_mode = 'rgb_array')\n",
    "\n",
    "# Agent and Model Hyperparameters\n",
    "ALPHA = 1e-3\n",
    "GAMMA = 0.995\n",
    "BATCH_SIZE = 64\n",
    "MAX_MEM_SIZE = 1000\n",
    "EPSILON_DECAY = 1e-4\n",
    "NUM_STEPS_FOR_LEARN = 4\n",
    "\n",
    "n_episodes = 10\n",
    "max_num_timesteps = 500 # per episode\n",
    "\n",
    "agent = SpaceInvaderAgent(learn_rate = ALPHA, gamma = GAMMA, num_actions = 6, epsilon = 1, batch_size = BATCH_SIZE, max_mem_size = MAX_MEM_SIZE, epsilon_decay = EPSILON_DECAY, num_steps_for_learn = NUM_STEPS_FOR_LEARN)\n",
    "\n",
    "total_point_history = []\n",
    "num_steps = []\n",
    "epsilon_history = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    start = time.time()\n",
    "    done = False\n",
    "    score = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        state_q = np.expand_dims(state, axis=0)\n",
    "        action = agent.choose_action(state_q)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        score = score + reward\n",
    "        agent.store_memory(state, action, reward, next_state, done, truncated)\n",
    "        state = next_state.copy()\n",
    "        agent.learn()\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    total_point_history.append(score)\n",
    "    num_steps.append(agent.learn_step_cntr)\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    \n",
    "    print(f\"\\rEpisode {i + 1} | Total points: {score:.2f} | Duration: {duration:.2f} | Total Steps: {agent.learn_step_cntr} | Current Epsilon: {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "e6f91ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run using random actions.\n",
    "\n",
    "# Create SpaceInvader gym environment with rgb_array mode.\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space = False, render_mode = 'human')\n",
    "\n",
    "env.action_space.seed(13)\n",
    "state, info = env.reset(seed=13)\n",
    "\n",
    "for _ in range(500):\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    if done or truncated:\n",
    "        state, info = env.reset()\n",
    "        \n",
    "env.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0f3e247f-6bb1-4b2d-8c3a-b4eb51e0bf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example run using trained agent to choose actions.\n",
    "\n",
    "# Create SpaceInvader gym environment with human render mode.\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space = False, render_mode = 'human')\n",
    "\n",
    "# Initialize environment.\n",
    "state, info = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    state_q = np.expand_dims(state, axis=0)\n",
    "    action = agent.choose_action(state_q)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        state, info = env.reset()\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f288eea-f1d8-471d-b928-5d32f34b6dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a8ca700c-7b4a-4261-8cb0-20e37796bfb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "agent = SpaceInvaderAgent(learn_rate = 1e-3, gamma = 0.99, num_actions = 6, epsilon = 0, batch_size = 30, max_mem_size = 10000, epsilon_decay = -0.01)\n",
    "state_q = np.expand_dims(state, axis=0)\n",
    "print(state_q.shape)\n",
    "action = agent.choose_action(state_q)\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "agent.store_memory(state, action, reward, next_state, done, truncated)\n",
    "\n",
    "for _ in range(30):\n",
    "    state = next_state\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    score = score + reward\n",
    "    agent.store_memory(state, action, reward, next_state, done, truncated)\n",
    "    \n",
    "\n",
    "states, actions, rewards, next_states, dones, truncateds = agent.memory.get_experiences(agent.batch_size)\n",
    "#agent.compute_loss(states, actions, rewards, next_states, dones, truncateds)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = agent.compute_loss(states, actions, rewards, next_states, dones, truncateds)\n",
    "gradients = tape.gradient(loss, agent.q_network.trainable_variables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
