{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "777dd059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPool2D, Flatten, GlobalAvgPool2D\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9428d2d-e4fe-4739-ad35-71d193cdb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "    - Install and experiment with tensorflow-GPU and CUDA.\n",
    "    - Experiment with different network architectures / hyperparameters.\n",
    "    - Train agent for much longer and many more steps...\n",
    "    - Train multiple models and compare with matplotlib graphs...\n",
    "    - Add way to save from and load models to agent.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3f0be9a5-27c3-454f-8a2e-9435e0423927",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.13.0'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8f966b57-266c-451f-a1f1-b7abca79e51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MemoryBuffer():\n",
    "    \"\"\"\n",
    "    Class used to hold the memory experiences of the agent. Data pertaining to each \"step\" of the environment is\n",
    "    contained within an \"experience\" tuple. A buffer holds these tuples, which the agent extracts during learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_mem_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_mem_size: (int) max size of memory buffer.\n",
    "            \n",
    "        Attributes:\n",
    "            experience: (namedtuple) represents a single \"experience\" / step of environment. Contains\n",
    "                the current state, action, reward, next state, and whether the episode is done or truncated.\n",
    "            buffer: deque used as structure to hold \"experiences\".\n",
    "        \"\"\"\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\", \"truncated\"])\n",
    "        self.buffer = deque(maxlen = max_mem_size)\n",
    "        \n",
    "    def store_memory(self,  state, action, reward, next_state, done, truncated):\n",
    "        \"\"\"\n",
    "        Adds a single \"experience\" to the memory buffer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.buffer.append(self.experience(state, action, reward, next_state, done, truncated))\n",
    "        \n",
    "    def get_experiences(self, batch_size):\n",
    "        \"\"\"\n",
    "        Extracts a random selection of experiences from the memory buffer. \n",
    "        \n",
    "        Args: \n",
    "            batch_size: (int) number of experiences to extract.\n",
    "            \n",
    "        Returns:\n",
    "            states: (numpy array) each element is a 210x60x3 array representing the current game image.\n",
    "            actions: (numpy array) array holding each action chosen for each corresponding step.\n",
    "            rewards: (numpy array) rewards gained from each corresponding step.\n",
    "            next_states: (numpy array) each element is a 210x60x3 array representing the next game image based on \n",
    "                the action taken.\n",
    "            dones: (numpy array) 1 if corresponding episode is finished, 0 otherwise.\n",
    "            truncateds: (numpy array) 1 if corresponding episode terminated, 0 otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        random_selection = random.sample(self.buffer, batch_size)\n",
    "        states = np.array([experience.state for experience in random_selection])\n",
    "        actions = np.array([experience.action for experience in random_selection])\n",
    "        rewards = np.array([experience.reward for experience in random_selection])\n",
    "        next_states = np.array([experience.next_state for experience in random_selection])\n",
    "        dones = np.array([experience.done for experience in random_selection])\n",
    "        truncateds = np.array([experience.truncated for experience in random_selection])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, truncateds\n",
    "        \n",
    "        \n",
    "class SpaceInvaderAgent():\n",
    "    \"\"\"\n",
    "    Represents an agent that can learn to play Atari Space Invaders using Deep-Q Learning. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learn_rate, gamma, num_actions, epsilon, batch_size, max_mem_size, epsilon_decay, num_steps_for_learn, epsilon_end = 0.01, update_target_net = 100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            TODO: Add args...\n",
    "            \n",
    "        Attributes:\n",
    "            TODO: Add attributes...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.actions = [i for i in range(num_actions)]\n",
    "        self.learn_rate = learn_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = MemoryBuffer(max_mem_size)\n",
    "        self.num_steps_for_learn = num_steps_for_learn\n",
    "        self.learn_step_cntr = 0\n",
    "        self.update_target_net = update_target_net\n",
    "        \n",
    "        self.q_network = Sequential([\n",
    "            Input(shape = (210, 160, 3)),\n",
    "            Conv2D(filters = 32, kernel_size = 3, activation = 'relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 64, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 128, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(num_actions, activation='linear')\n",
    "            ])\n",
    "        \n",
    "        self.q_target = Sequential([\n",
    "            Input(shape = (210, 160, 3)),\n",
    "            Conv2D(filters = 32, kernel_size = 3, activation = 'relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 64, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Conv2D(filters = 128, kernel_size= 3, activation='relu', padding = 'same'),\n",
    "            MaxPool2D(),\n",
    "            Flatten(),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(num_actions, activation='linear')\n",
    "            ])\n",
    "        \n",
    "        self.optimizer = Adam(learning_rate = self.learn_rate)\n",
    "        self.q_network.compile()\n",
    "        self.q_target.compile()\n",
    "                               \n",
    "        \n",
    "    def store_memory(self, state, action, reward, next_state, done, truncated):\n",
    "        \"\"\"\n",
    "        Adds a single \"experience\" to the agent's MemoryBuffer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.memory.store_memory(state, action, reward, next_state, done, truncated)\n",
    "                              \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"\n",
    "        Chooses an action based on the agent's greedy-epsilon policy. Possible actions include: \n",
    "        0 (NOOP), 1 (FIRE), 2 (RIGHT), 3 (LEFT), 4 (RIGHTFIRE), 5 (LEFTFIRE)\n",
    "        \n",
    "        Args:\n",
    "            TODO: add state arg...\n",
    "            \n",
    "        Returns: \n",
    "            action: (int) representing one of the six possible actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pick a random action if generated random number is less than epsilon.\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        # Use Q-Network to pick action based on state.\n",
    "        else:\n",
    "            q_values = self.q_network(state)\n",
    "            action = tf.math.argmax(q_values, axis = 1).numpy()[0]\n",
    "                              \n",
    "        return action\n",
    "                              \n",
    "    def compute_loss(self, states, actions, rewards, next_states, dones, truncateds):\n",
    "        \"\"\"\n",
    "        Calculates the loss for the Deep-Q Network.\n",
    "\n",
    "        Args:\n",
    "            states: (numpy array) each element is a 210x60x3 array representing the current game image.\n",
    "            actions: (numpy array) array holding each action chosen for each corresponding step.\n",
    "            rewards: (numpy array) rewards gained from each corresponding step.\n",
    "            next_states: (numpy array) each element is a 210x60x3 array representing the next game image based on \n",
    "                the action taken.\n",
    "            dones: (numpy array) 1 if corresponding episode is finished, 0 otherwise.\n",
    "            truncateds: (numpy array) 1 if corresponding episode terminated, 0 otherwise.  \n",
    "\n",
    "        Returns:\n",
    "            loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between the y targets and the Q(s,a) values.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Determine max-Q(s, a) from next states and calculate y-targets using the target Q-Network.\n",
    "        max_qsa = tf.reduce_max(self.q_target(next_states), axis=-1)\n",
    "        y_targets = rewards + (1 - dones) * (1 - truncateds) * self.gamma * max_qsa\n",
    "        \n",
    "        # Determine Q-values from chosen actions using the original Q-Network.\n",
    "        q_values = self.q_network(states)\n",
    "        q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]), tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "        # Calculate the loss from the Mean-Squared Error between the y targest and Q-values.\n",
    "        loss = MSE(y_targets, q_values)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Updates the trainable weights of the Q-Network based on the Mean-Sqaured Error loss function. Learning only\n",
    "        takes place every \"num_steps_for_learn\" steps. Will not learn unless the MemoryBuffer contains enough\n",
    "        experiences for a full sample batch (batch_size). Also decrements the agent's epsilon attribute.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Update learn step counter.\n",
    "        self.learn_step_cntr += 1\n",
    "        \n",
    "        # Decrement epsilon.\n",
    "        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.epsilon_end else self.epsilon\n",
    "        \n",
    "        # Check if it is time to learn.\n",
    "        if self.learn_step_cntr % self.num_steps_for_learn != 0:\n",
    "            return\n",
    "        \n",
    "        # Make sure MemoryBuffer has enough experiences for full batch.\n",
    "        if len(self.memory.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Update target Q-Network based on step count.\n",
    "        if self.learn_step_cntr % self.update_target_net == 0:\n",
    "            self.q_target.set_weights(self.q_network.get_weights())\n",
    "                              \n",
    "        # Extract experiences from MemoryBuffer.    \n",
    "        states, actions, rewards, next_states, dones, truncateds = self.memory.get_experiences(self.batch_size)\n",
    "            \n",
    "        # Compute loss with GradientTape.    \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(states, actions, rewards, next_states, dones, truncateds)\n",
    "\n",
    "        # Compute the gradients of the loss with respect to the weights.\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "\n",
    "        # Update the weights of the q_network.\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "                              \n",
    "    \n",
    "    def save_model(self):\n",
    "        #TODO: Add way to save q_network.\n",
    "        return\n",
    "    \n",
    "    def load_model(self):\n",
    "        #TODO: Add way to load an existing q_network.\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e535550b-1806-4453-bc7f-474b09e8fd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 | Total points: 65.00 | Duration: 135.12 | Total Steps: 365 | Current Epsilon: 1.00\n",
      "Episode 2 | Total points: 160.00 | Duration: 234.77 | Total Steps: 895 | Current Epsilon: 0.99\n",
      "Episode 3 | Total points: 110.00 | Duration: 202.99 | Total Steps: 1348 | Current Epsilon: 0.99\n",
      "Episode 4 | Total points: 40.00 | Duration: 123.90 | Total Steps: 1628 | Current Epsilon: 0.98\n",
      "Episode 5 | Total points: 50.00 | Duration: 163.71 | Total Steps: 2000 | Current Epsilon: 0.98\n",
      "Episode 6 | Total points: 120.00 | Duration: 209.32 | Total Steps: 2474 | Current Epsilon: 0.98\n",
      "Episode 7 | Total points: 270.00 | Duration: 346.56 | Total Steps: 3252 | Current Epsilon: 0.97\n",
      "Episode 8 | Total points: 170.00 | Duration: 217.62 | Total Steps: 3744 | Current Epsilon: 0.96\n",
      "Episode 9 | Total points: 455.00 | Duration: 367.23 | Total Steps: 4572 | Current Epsilon: 0.95\n",
      "Episode 10 | Total points: 65.00 | Duration: 154.39 | Total Steps: 4920 | Current Epsilon: 0.95\n",
      "Episode 11 | Total points: 180.00 | Duration: 230.48 | Total Steps: 5442 | Current Epsilon: 0.95\n",
      "Episode 12 | Total points: 65.00 | Duration: 145.61 | Total Steps: 5771 | Current Epsilon: 0.94\n",
      "Episode 13 | Total points: 135.00 | Duration: 228.59 | Total Steps: 6284 | Current Epsilon: 0.94\n",
      "Episode 14 | Total points: 90.00 | Duration: 180.93 | Total Steps: 6692 | Current Epsilon: 0.93\n",
      "Episode 15 | Total points: 30.00 | Duration: 166.77 | Total Steps: 7071 | Current Epsilon: 0.93\n",
      "Episode 16 | Total points: 75.00 | Duration: 132.94 | Total Steps: 7368 | Current Epsilon: 0.93\n",
      "Episode 17 | Total points: 45.00 | Duration: 186.60 | Total Steps: 7791 | Current Epsilon: 0.92\n",
      "Episode 18 | Total points: 70.00 | Duration: 191.40 | Total Steps: 8222 | Current Epsilon: 0.92\n",
      "Episode 19 | Total points: 155.00 | Duration: 222.23 | Total Steps: 8721 | Current Epsilon: 0.91\n",
      "Episode 20 | Total points: 35.00 | Duration: 135.23 | Total Steps: 9025 | Current Epsilon: 0.91\n",
      "Episode 21 | Total points: 260.00 | Duration: 258.90 | Total Steps: 9610 | Current Epsilon: 0.90\n",
      "Episode 22 | Total points: 125.00 | Duration: 241.05 | Total Steps: 10152 | Current Epsilon: 0.90\n",
      "Episode 23 | Total points: 75.00 | Duration: 174.94 | Total Steps: 10548 | Current Epsilon: 0.89\n",
      "Episode 24 | Total points: 155.00 | Duration: 210.52 | Total Steps: 11026 | Current Epsilon: 0.89\n",
      "Episode 25 | Total points: 20.00 | Duration: 205.59 | Total Steps: 11490 | Current Epsilon: 0.89\n",
      "Episode 26 | Total points: 205.00 | Duration: 253.15 | Total Steps: 12061 | Current Epsilon: 0.88\n",
      "Episode 27 | Total points: 105.00 | Duration: 256.92 | Total Steps: 12643 | Current Epsilon: 0.87\n",
      "Episode 28 | Total points: 125.00 | Duration: 189.66 | Total Steps: 13070 | Current Epsilon: 0.87\n",
      "Episode 29 | Total points: 60.00 | Duration: 127.26 | Total Steps: 13358 | Current Epsilon: 0.87\n",
      "Episode 30 | Total points: 30.00 | Duration: 120.52 | Total Steps: 13628 | Current Epsilon: 0.86\n",
      "Episode 31 | Total points: 0.00 | Duration: 156.56 | Total Steps: 13980 | Current Epsilon: 0.86\n",
      "Episode 32 | Total points: 200.00 | Duration: 358.87 | Total Steps: 14791 | Current Epsilon: 0.85\n",
      "Episode 33 | Total points: 215.00 | Duration: 265.98 | Total Steps: 15389 | Current Epsilon: 0.85\n",
      "Episode 34 | Total points: 215.00 | Duration: 279.39 | Total Steps: 16016 | Current Epsilon: 0.84\n",
      "Episode 35 | Total points: 120.00 | Duration: 242.22 | Total Steps: 16563 | Current Epsilon: 0.83\n",
      "Episode 36 | Total points: 45.00 | Duration: 163.81 | Total Steps: 16931 | Current Epsilon: 0.83\n",
      "Episode 37 | Total points: 225.00 | Duration: 266.55 | Total Steps: 17531 | Current Epsilon: 0.82\n",
      "Episode 38 | Total points: 65.00 | Duration: 129.23 | Total Steps: 17821 | Current Epsilon: 0.82\n",
      "Episode 39 | Total points: 135.00 | Duration: 266.43 | Total Steps: 18421 | Current Epsilon: 0.82\n",
      "Episode 40 | Total points: 385.00 | Duration: 356.29 | Total Steps: 19223 | Current Epsilon: 0.81\n",
      "Episode 41 | Total points: 210.00 | Duration: 284.36 | Total Steps: 19863 | Current Epsilon: 0.80\n",
      "Episode 42 | Total points: 80.00 | Duration: 156.61 | Total Steps: 20213 | Current Epsilon: 0.80\n",
      "Episode 43 | Total points: 135.00 | Duration: 220.71 | Total Steps: 20708 | Current Epsilon: 0.79\n",
      "Episode 44 | Total points: 180.00 | Duration: 245.71 | Total Steps: 21263 | Current Epsilon: 0.79\n",
      "Episode 45 | Total points: 285.00 | Duration: 310.76 | Total Steps: 21963 | Current Epsilon: 0.78\n",
      "Episode 46 | Total points: 65.00 | Duration: 199.39 | Total Steps: 22410 | Current Epsilon: 0.78\n",
      "Episode 47 | Total points: 340.00 | Duration: 445.31 | Total Steps: 23410 | Current Epsilon: 0.77\n",
      "Episode 48 | Total points: 120.00 | Duration: 215.39 | Total Steps: 23894 | Current Epsilon: 0.76\n",
      "Episode 49 | Total points: 205.00 | Duration: 271.03 | Total Steps: 24501 | Current Epsilon: 0.75\n",
      "Episode 50 | Total points: 120.00 | Duration: 234.67 | Total Steps: 25031 | Current Epsilon: 0.75\n",
      "Episode 51 | Total points: 180.00 | Duration: 213.75 | Total Steps: 25510 | Current Epsilon: 0.74\n",
      "Episode 52 | Total points: 45.00 | Duration: 224.25 | Total Steps: 26014 | Current Epsilon: 0.74\n",
      "Episode 53 | Total points: 20.00 | Duration: 126.59 | Total Steps: 26298 | Current Epsilon: 0.74\n",
      "Episode 54 | Total points: 190.00 | Duration: 205.50 | Total Steps: 26756 | Current Epsilon: 0.73\n",
      "Episode 55 | Total points: 180.00 | Duration: 157.04 | Total Steps: 27107 | Current Epsilon: 0.73\n",
      "Episode 56 | Total points: 80.00 | Duration: 206.35 | Total Steps: 27567 | Current Epsilon: 0.72\n",
      "Episode 57 | Total points: 215.00 | Duration: 274.01 | Total Steps: 28177 | Current Epsilon: 0.72\n",
      "Episode 58 | Total points: 55.00 | Duration: 170.87 | Total Steps: 28560 | Current Epsilon: 0.71\n",
      "Episode 59 | Total points: 100.00 | Duration: 136.98 | Total Steps: 28870 | Current Epsilon: 0.71\n",
      "Episode 60 | Total points: 90.00 | Duration: 237.05 | Total Steps: 29400 | Current Epsilon: 0.71\n",
      "Episode 61 | Total points: 115.00 | Duration: 171.38 | Total Steps: 29785 | Current Epsilon: 0.70\n",
      "Episode 62 | Total points: 75.00 | Duration: 175.44 | Total Steps: 30176 | Current Epsilon: 0.70\n",
      "Episode 63 | Total points: 260.00 | Duration: 385.85 | Total Steps: 31041 | Current Epsilon: 0.69\n",
      "Episode 64 | Total points: 210.00 | Duration: 357.01 | Total Steps: 31841 | Current Epsilon: 0.68\n",
      "Episode 65 | Total points: 330.00 | Duration: 444.16 | Total Steps: 32841 | Current Epsilon: 0.67\n",
      "Episode 66 | Total points: 210.00 | Duration: 445.08 | Total Steps: 33841 | Current Epsilon: 0.66\n",
      "Episode 67 | Total points: 135.00 | Duration: 230.32 | Total Steps: 34359 | Current Epsilon: 0.66\n",
      "Episode 68 | Total points: 215.00 | Duration: 328.05 | Total Steps: 35092 | Current Epsilon: 0.65\n",
      "Episode 69 | Total points: 155.00 | Duration: 199.38 | Total Steps: 35540 | Current Epsilon: 0.64\n",
      "Episode 70 | Total points: 30.00 | Duration: 133.78 | Total Steps: 35843 | Current Epsilon: 0.64\n",
      "Episode 71 | Total points: 320.00 | Duration: 207.11 | Total Steps: 36306 | Current Epsilon: 0.64\n",
      "Episode 72 | Total points: 105.00 | Duration: 180.44 | Total Steps: 36709 | Current Epsilon: 0.63\n",
      "Episode 73 | Total points: 285.00 | Duration: 300.80 | Total Steps: 37386 | Current Epsilon: 0.63\n",
      "Episode 74 | Total points: 45.00 | Duration: 135.22 | Total Steps: 37688 | Current Epsilon: 0.62\n",
      "Episode 75 | Total points: 105.00 | Duration: 163.87 | Total Steps: 38056 | Current Epsilon: 0.62\n"
     ]
    }
   ],
   "source": [
    "# Create SpaceInvader gym environment with rgb_array mode.\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space = False, render_mode = 'rgb_array')\n",
    "\n",
    "# Agent and Model Hyperparameters\n",
    "ALPHA = 1e-3\n",
    "GAMMA = 0.995\n",
    "BATCH_SIZE = 64\n",
    "MAX_MEM_SIZE = 1000\n",
    "EPSILON_DECAY = 1e-5\n",
    "NUM_STEPS_FOR_LEARN = 4\n",
    "\n",
    "n_episodes = 75\n",
    "max_num_timesteps = 1000 # per episode\n",
    "\n",
    "agent = SpaceInvaderAgent(learn_rate = ALPHA, gamma = GAMMA, num_actions = 6, epsilon = 1, batch_size = BATCH_SIZE, max_mem_size = MAX_MEM_SIZE, epsilon_decay = EPSILON_DECAY, num_steps_for_learn = NUM_STEPS_FOR_LEARN)\n",
    "\n",
    "total_point_history = []\n",
    "num_steps = []\n",
    "epsilon_history = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    start = time.time()\n",
    "    done = False\n",
    "    score = 0\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    for t in range(max_num_timesteps):\n",
    "        state_q = np.expand_dims(state, axis=0)\n",
    "        action = agent.choose_action(state_q)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        score = score + reward\n",
    "        agent.store_memory(state, action, reward, next_state, done, truncated)\n",
    "        state = next_state.copy()\n",
    "        agent.learn()\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    end = time.time()\n",
    "    duration = end - start\n",
    "    total_point_history.append(score)\n",
    "    num_steps.append(agent.learn_step_cntr)\n",
    "    epsilon_history.append(agent.epsilon)\n",
    "    \n",
    "    print(f\"\\rEpisode {i + 1} | Total points: {score:.2f} | Duration: {duration:.2f} | Total Steps: {agent.learn_step_cntr} | Current Epsilon: {agent.epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "44c6252a-b516-4b4b-9ba6-129c589d8a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_222\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_361 (Conv2D)         (None, 210, 160, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_360 (MaxPool  (None, 105, 80, 32)       0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " conv2d_362 (Conv2D)         (None, 105, 80, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_361 (MaxPool  (None, 52, 40, 64)        0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " conv2d_363 (Conv2D)         (None, 52, 40, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_362 (MaxPool  (None, 26, 20, 128)       0         \n",
      " ing2D)                                                          \n",
      "                                                                 \n",
      " flatten_315 (Flatten)       (None, 66560)             0         \n",
      "                                                                 \n",
      " dense_1027 (Dense)          (None, 64)                4259904   \n",
      "                                                                 \n",
      " dense_1028 (Dense)          (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1029 (Dense)          (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4357702 (16.62 MB)\n",
      "Trainable params: 4357702 (16.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.q_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "e6f91ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run using random actions.\n",
    "\n",
    "# Create SpaceInvader gym environment with rgb_array mode.\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space = False, render_mode = 'human')\n",
    "\n",
    "env.action_space.seed(13)\n",
    "state, info = env.reset(seed=13)\n",
    "\n",
    "for _ in range(500):\n",
    "    state, reward, done, truncated, info = env.step(env.action_space.sample())\n",
    "    if done or truncated:\n",
    "        state, info = env.reset()\n",
    "        \n",
    "env.close()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "0f3e247f-6bb1-4b2d-8c3a-b4eb51e0bf6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example run using trained agent to choose actions.\n",
    "\n",
    "# Create SpaceInvader gym environment with human render mode.\n",
    "env = gym.make(\"ALE/SpaceInvaders-v5\", full_action_space = False, render_mode = 'human')\n",
    "\n",
    "# Initialize environment.\n",
    "state, info = env.reset()\n",
    "\n",
    "for _ in range(500):\n",
    "    state_q = np.expand_dims(state, axis=0)\n",
    "    action = agent.choose_action(state_q)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        state, info = env.reset()\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f288eea-f1d8-471d-b928-5d32f34b6dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a8ca700c-7b4a-4261-8cb0-20e37796bfb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "score = 0\n",
    "state, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "agent = SpaceInvaderAgent(learn_rate = 1e-3, gamma = 0.99, num_actions = 6, epsilon = 0, batch_size = 30, max_mem_size = 10000, epsilon_decay = -0.01)\n",
    "state_q = np.expand_dims(state, axis=0)\n",
    "print(state_q.shape)\n",
    "action = agent.choose_action(state_q)\n",
    "next_state, reward, terminated, truncated, info = env.step(action)\n",
    "agent.store_memory(state, action, reward, next_state, done, truncated)\n",
    "\n",
    "for _ in range(30):\n",
    "    state = next_state\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "    score = score + reward\n",
    "    agent.store_memory(state, action, reward, next_state, done, truncated)\n",
    "    \n",
    "\n",
    "states, actions, rewards, next_states, dones, truncateds = agent.memory.get_experiences(agent.batch_size)\n",
    "#agent.compute_loss(states, actions, rewards, next_states, dones, truncateds)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = agent.compute_loss(states, actions, rewards, next_states, dones, truncateds)\n",
    "gradients = tape.gradient(loss, agent.q_network.trainable_variables)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
